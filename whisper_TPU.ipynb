{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bols-blue/google_colab/blob/main/whisper_TPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmXxxQ-manoj"
      },
      "outputs": [],
      "source": [
        "! apt-get install -y librosa-dev chex\n",
        "!python -V\n",
        "\n",
        "!pip install -qq ipython==7.34.0\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install git+https://github.com/bols-blue-org/langchain_sample.git\n",
        "\n",
        "!pip uninstall -y numpy\n",
        "!pip uninstall -y numba\n",
        "!pip install numba\n",
        "\n",
        "!pip install kora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5_flVeAIjCe"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from kora.xattr import get_id\n",
        "\n",
        "id = get_id(\"/content/drive/MyDrive/Meet Recordings\")\n",
        "print(id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxSd54WTUAct"
      },
      "outputs": [],
      "source": [
        "!ls \"/content/drive/MyDrive/Meet Recordings\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oywcqskoptrt"
      },
      "source": [
        "エラーが出る場合該当部分を削除する。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJugrN2IQl1O"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "import os\n",
        "import mimetypes\n",
        "from google.colab import runtime\n",
        "\n",
        "lang = \"ja\"\n",
        "model = whisper.load_model(\"large\")\n",
        "\n",
        "def convert_txt(file_path,dirname, mock=False):\n",
        "  basename = os.path.basename(file_path)\n",
        "  print(file_path)\n",
        "  output_file = f\"{dirname}/{basename}.txt\"\n",
        "  if mock == True:\n",
        "    return\n",
        "  is_file = os.path.isfile(output_file)\n",
        "  if is_file:\n",
        "    print(f\"{output_file} は存在します。処理をスキップします。\")\n",
        "    return\n",
        "  # Load audio\n",
        "  audio = whisper.load_audio(f\"{dirname}/{basename}\")\n",
        "\n",
        "  result = model.transcribe(audio, verbose=True, language=lang)\n",
        "  # Write into a text file\n",
        "  with open(f\"{dirname}/{basename}.txt\", \"w\") as f:\n",
        "    f.write(result[\"text\"])\n",
        "\n",
        "dirname = '/content/drive/MyDrive/Meet Recordings'\n",
        "\n",
        "files = os.listdir(path=dirname)\n",
        "for name in files:\n",
        "  file_type = mimetypes.guess_type(name)\n",
        "  print(file_type)\n",
        "  if file_type[0] == 'video/mp4':\n",
        "    # convert_txt(name, mock=True)\n",
        "    convert_txt(name, dirname)\n",
        "\n",
        "dirname = '/content/drive/MyDrive/Meet Recordings naitou'\n",
        "\n",
        "files = os.listdir(path=dirname)\n",
        "for name in files:\n",
        "  file_type = mimetypes.guess_type(name)\n",
        "  print(file_type)\n",
        "  if file_type[0] == 'video/mp4':\n",
        "    # convert_txt(name, mock=True)\n",
        "    convert_txt(name, dirname)\n",
        "\n",
        "runtime.unassign()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ykb1nol9X0N"
      },
      "outputs": [],
      "source": [
        "\n",
        "import requests\n",
        "import json\n",
        "import datetime\n",
        "\n",
        "import os\n",
        "import mimetypes\n",
        "\n",
        "path='/content/drive/MyDrive/Meet Recordings'\n",
        "\n",
        "\n",
        "from langchain_sample.send_notion import create_meeting_log\n",
        "\n",
        "\n",
        "create_meeting_log(\"テスト\")\n",
        "files = os.listdir(path)\n",
        "\n",
        "\n",
        "for name in files:\n",
        "  file_type = mimetypes.guess_type(name)\n",
        "  print(file_type)\n",
        "  if file_type[0] == \"text/plain\":\n",
        "    create_meeting_log(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kndtYvurPuMn"
      },
      "outputs": [],
      "source": [
        "!pip install langchain openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA6dBtXpPV7y"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "# チャットモデルのラッパーをインポート\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# LLMChain をインポート\n",
        "from langchain import LLMChain, OpenAI\n",
        "\n",
        "# チャットプロンプト用のテンプレートをインポート\n",
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain.prompts.chat import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "path='/content/drive/MyDrive/Meet Recordings'\n",
        "\n",
        "files = os.listdir(path)\n",
        "\n",
        "data = ''\n",
        "\n",
        "for name in files:\n",
        "  file_type = mimetypes.guess_type(name)\n",
        "  print(file_type)\n",
        "  if file_type[0] == \"text/plain\":\n",
        "    f = open(path+name, 'r')\n",
        "    data = f.read()\n",
        "    f.close()\n",
        "    create_mtg_log(data)\n",
        "\n",
        "# チャットモデルのラッパーを初期化\n",
        "from langchain.schema import LLMResult\n",
        "\n",
        "chat = ChatOpenAI(temperature=0.7)\n",
        "\n",
        "# memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "memory = ConversationSummaryMemory(llm=OpenAI())\n",
        "\n",
        "# SystemMessage 用のテンプレートの作成\n",
        "template=\"あなたは優秀なドローンのエンジニアです\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "\n",
        "# HumanMessage 用のテンプレートの作成\n",
        "human_template=\"議事録を作成したいので出てきた会議のトピックに関して箇条書きで出力してください。「{text}」\"\n",
        "human_message_prompt1 = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "# HumanMessage 用のテンプレートの作成\n",
        "human_template=\"このアイデアの独自の価値提案と分かりやすいコンセプトを考えてください。出力は箇条書きでお願いします「{text}」\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "batch_messages = [\n",
        "    [\n",
        "        system_message_prompt,\n",
        "        human_message_prompt1\n",
        "    ],\n",
        "    [\n",
        "        system_message_prompt,\n",
        "        human_message_prompt\n",
        "    ],\n",
        "]\n",
        "# チャットモデルにメッセージを渡して、予測を受け取る\n",
        "result: LLMResult = chat.generate(batch_messages)\n",
        "\n",
        "# 予測結果を表示する\n",
        "for generations in result.generations:\n",
        "    for generation in generations:\n",
        "        print(generation.text,\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM/FflZwofW1TJAOZ74g8xP",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}